{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0347389",
   "metadata": {},
   "source": [
    "# MPS Tutorial\n",
    "This sample will show you how to use the Aria MPS data via the MPS apis.\n",
    "Please refer to the MPS wiki for more information about data formats and schemas\n",
    "\n",
    "### Notebook stuck?\n",
    "Note that because of Jupyter and Plotly issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "163e63fa",
   "metadata": {},
   "source": [
    "## Download the MPS sample dataset locally\n",
    "> The sample dataset will get downloaded to a **tmp** folder by default. Please modify the path if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa38162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "\n",
    "google_colab_env = 'google.colab' in str(get_ipython())\n",
    "if google_colab_env:\n",
    "    print(\"Running from Google Colab, installing projectaria_tools and getting sample data\")\n",
    "    !pip install projectaria-tools\n",
    "    mps_sample_path = \"./mps_sample_data/\"\n",
    "else:\n",
    "    mps_sample_path = \"/tmp/mps_sample_data/\"\n",
    "\n",
    "base_url = \"https://www.projectaria.com/async/sample/download/?bucket=mps&filename=\"\n",
    "os.makedirs(mps_sample_path, exist_ok=True)\n",
    "\n",
    "filenames = [\n",
    "    \"sample.vrs\",\n",
    "    \"slam_v1_1_0.zip\",\n",
    "    \"eye_gaze_v3_1_0.zip\",\n",
    "    \"hand_tracking_v2_0_0.zip\"]\n",
    "\n",
    "print(\"Downloading sample data\")\n",
    "for filename in tqdm(filenames):\n",
    "    print(f\"Processing: {filename}\")\n",
    "    full_path: str = os.path.join(mps_sample_path, filename)\n",
    "    urlretrieve(f\"{base_url}{filename}\", full_path)\n",
    "    if filename.endswith(\".zip\"):\n",
    "        with ZipFile(full_path, 'r') as zip_ref:\n",
    "            folder_extraction = mps_sample_path\n",
    "            os.makedirs(folder_extraction, exist_ok=True)\n",
    "            zip_ref.extractall(path=folder_extraction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44984456",
   "metadata": {},
   "source": [
    "## Load the trajectory, point cloud, eye gaze and hands using the MPS apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider, mps\n",
    "from projectaria_tools.core.mps.utils import (\n",
    "    filter_points_from_confidence,\n",
    "    get_gaze_vector_reprojection,\n",
    "    get_nearest_eye_gaze,\n",
    "    get_nearest_pose,\n",
    ")\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "import numpy as np\n",
    "\n",
    "# Load the VRS file\n",
    "vrsfile = os.path.join(mps_sample_path, \"sample.vrs\")\n",
    "\n",
    "# Trajectory, global points, and online calibration\n",
    "closed_loop_trajectory = os.path.join(\n",
    "    mps_sample_path, \"slam\", \"closed_loop_trajectory.csv\"\n",
    ")\n",
    "global_points = os.path.join(mps_sample_path, \"slam\", \"semidense_points.csv.gz\")\n",
    "online_calibrations_path = os.path.join(mps_sample_path, \"slam\", \"online_calibration.jsonl\")\n",
    "\n",
    "# Eye gaze\n",
    "generalized_eye_gaze_path = os.path.join(\n",
    "    mps_sample_path, \"eye_gaze\", \"general_eye_gaze.csv\"\n",
    ")\n",
    "calibrated_eye_gaze_path = os.path.join(\n",
    "    mps_sample_path, \"eye_gaze\", \"personalized_eye_gaze.csv\"\n",
    ")\n",
    "\n",
    "# Hand tracking\n",
    "hand_tracking_results_path = os.path.join(\n",
    "    mps_sample_path, \"hand_tracking\", \"hand_tracking_results.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create data provider and get T_device_rgb\n",
    "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
    "# Since we want to display the position of the RGB camera, we are querying its relative location\n",
    "# from the device and will apply it to the device trajectory.\n",
    "T_device_RGB = provider.get_device_calibration().get_transform_device_sensor(\n",
    "    \"camera-rgb\"\n",
    ")\n",
    "\n",
    "## Load trajectory and global points\n",
    "mps_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory)\n",
    "points = mps.read_global_point_cloud(global_points)\n",
    "\n",
    "## Load online calibration file\n",
    "online_calibrations = mps.read_online_calibration(online_calibrations_path)\n",
    "\n",
    "## Load eyegaze\n",
    "generalized_eye_gazes = mps.read_eyegaze(generalized_eye_gaze_path)\n",
    "calibrated_eye_gazes = mps.read_eyegaze(calibrated_eye_gaze_path)\n",
    "\n",
    "## Load hand tracking\n",
    "hand_tracking_results = mps.hand_tracking.read_hand_tracking_results(\n",
    "    hand_tracking_results_path\n",
    ")\n",
    "\n",
    "# Loaded data must be not empty\n",
    "assert(\n",
    "    len(mps_trajectory) != 0 and\n",
    "    len(points) != 0 and\n",
    "    len(online_calibrations) !=0 and\n",
    "    len(generalized_eye_gazes) != 0 and\n",
    "    len(calibrated_eye_gazes) != 0 and\n",
    "    len(hand_tracking_results) != 0)\n",
    "\n",
    "\n",
    "# Or you can load the whole mps output with MpsDataProvider\n",
    "mps_data_provider = mps.MpsDataProvider(mps.MpsDataPathsProvider(mps_sample_path).get_data_paths())\n",
    "\n",
    "assert(mps_data_provider.has_general_eyegaze() and\n",
    "      mps_data_provider.has_personalized_eyegaze() and \n",
    "      mps_data_provider.has_open_loop_poses() and\n",
    "      mps_data_provider.has_closed_loop_poses() and\n",
    "      mps_data_provider.has_online_calibrations() and\n",
    "      mps_data_provider.has_semidense_point_cloud() and\n",
    "      mps_data_provider.has_hand_tracking_results())\n",
    "\n",
    "# Get the MPS service versions\n",
    "print(f\"slam_version: {mps_data_provider.get_slam_version()}\")\n",
    "print(f\"eyegaze_version: {mps_data_provider.get_eyegaze_version()}\")\n",
    "print(f\"hand_tracking_version: {mps_data_provider.get_hand_tracking_version()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2d6248",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helper function to build the frustum\n",
    "def build_camera_frustum(transform_world_device):\n",
    "    points = (\n",
    "        np.array(\n",
    "            [[0, 0, 0], [0.5, 0.5, 1], [-0.5, 0.5, 1], [-0.5, -0.5, 1], [0.5, -0.5, 1]]\n",
    "        )\n",
    "        * 0.6\n",
    "    )\n",
    "    transform_world_rgb = transform_world_device @ T_device_RGB\n",
    "    points_transformed = transform_world_rgb @ points.transpose()\n",
    "    return go.Mesh3d(\n",
    "        x=points_transformed[0, :],\n",
    "        y=points_transformed[1, :],\n",
    "        z=points_transformed[2, :],\n",
    "        i=[0, 0, 0, 0, 1, 1],\n",
    "        j=[1, 2, 3, 4, 2, 3],\n",
    "        k=[2, 3, 4, 1, 3, 4],\n",
    "        showscale=False,\n",
    "        visible=False,\n",
    "        colorscale=\"jet\",\n",
    "        intensity=points[:, 2],\n",
    "        opacity=1.0,\n",
    "        hoverinfo=\"none\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e125bbdc",
   "metadata": {},
   "source": [
    "## Visualize the trajectory and point cloud in a 3D interactive plot\n",
    "* Load trajectory\n",
    "* Load global point cloud\n",
    "* Render dense trajectory (1Khz) as points.\n",
    "* Render subsampled 6DOF poses via camera frustum. Use calibration to transform RGB camera pose to world frame\n",
    "* Render subsampled point cloud\n",
    "\n",
    "_Please wait a minute for all the data to load. Zoom in to the point cloud and adjust your view. Then use the time slider to move the camera_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all world positions from the trajectory\n",
    "traj = np.empty([len(mps_trajectory), 3])\n",
    "for i in range(len(mps_trajectory)):\n",
    "    traj[i, :] = mps_trajectory[i].transform_world_device.translation()\n",
    "\n",
    "# Subsample trajectory for quick display\n",
    "skip = 1000\n",
    "mps_trajectory_subset = mps_trajectory[::skip]\n",
    "steps = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "# Load each pose as a camera frustum trace\n",
    "cam_frustums = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "for i in range(len(mps_trajectory_subset)):\n",
    "    pose = mps_trajectory_subset[i]\n",
    "    cam_frustums[i] = build_camera_frustum(pose.transform_world_device)\n",
    "    timestamp = pose.tracking_timestamp.total_seconds()\n",
    "    step = dict(method=\"update\", args=[{\"visible\": [False] * len(cam_frustums) + [True] * 2}, {\"title\": \"Trajectory and Point Cloud\"},], label=timestamp,)\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps[i] = step\n",
    "cam_frustums[0].visible = True\n",
    "\n",
    "# Filter the point cloud by inv depth and depth and load\n",
    "points = filter_points_from_confidence(points)\n",
    "# Retrieve point position\n",
    "point_cloud = np.stack([it.position_world for it in points])\n",
    "\n",
    "# Create slider to allow scrubbing and set the layout\n",
    "sliders = [dict(currentvalue={\"suffix\": \" s\", \"prefix\": \"Time :\"}, pad={\"t\": 5}, steps=steps,)]\n",
    "layout = go.Layout(\n",
    "    sliders=sliders, \n",
    "    scene=dict(\n",
    "        bgcolor='lightgray', \n",
    "        dragmode='orbit', \n",
    "        aspectmode='data', \n",
    "        xaxis_visible=False, \n",
    "        yaxis_visible=False,\n",
    "        zaxis_visible=False,\n",
    "        camera=dict(\n",
    "        eye=dict(x=0.5, y=0.5, z=0.5),\n",
    "        center=dict(x=0, y=0, z=0),\n",
    "        up=dict(x=0, y=0, z=1)\n",
    "    )),\n",
    "    width=1100,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "# Plot trajectory and point cloud\n",
    "# We color the points by their z coordinate\n",
    "trajectory = go.Scatter3d(x=traj[:, 0], y=traj[:, 1], z=traj[:, 2], mode=\"markers\", marker={\"size\": 2, \"opacity\": 0.8, \"color\": \"red\"}, name=\"Trajectory\", hoverinfo='none')\n",
    "global_points = go.Scatter3d(x=point_cloud[:, 0], y=point_cloud[:, 1], z=point_cloud[:, 2], mode=\"markers\",\n",
    "    marker={\"size\" : 1.5, \"color\": point_cloud[:, 2], \"cmin\": -1.5, \"cmax\": 2, \"colorscale\": \"viridis\",},\n",
    "    name=\"Global Points\", hoverinfo='none')\n",
    "\n",
    "# draw\n",
    "plot_figure = go.Figure(data=cam_frustums + [trajectory, global_points], layout=layout)\n",
    "plot_figure.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f37caf18",
   "metadata": {},
   "source": [
    "## Visualize generalized and calibrated eye gaze projection on an rgb image.\n",
    "* Load Eyegaze MPS output\n",
    "* Select a random RGB frame\n",
    "* Find the closest eye gaze data for the RGB frame\n",
    "* Project the eye gaze for the RGB frame by **using a fixed depth of 1m** or existing depth if available.\n",
    "* Show the gaze cross on the RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "rgb_stream_label = provider.get_label_from_stream_id(rgb_stream_id)\n",
    "num_rgb_frames = provider.get_num_data(rgb_stream_id)\n",
    "rgb_frame = provider.get_image_data_by_index(rgb_stream_id, (int)(num_rgb_frames-5))\n",
    "assert rgb_frame[0] is not None, \"no rgb frame\"\n",
    "\n",
    "image = rgb_frame[0].to_numpy_array()\n",
    "capture_timestamp_ns = rgb_frame[1].capture_timestamp_ns\n",
    "generalized_eye_gaze = get_nearest_eye_gaze(generalized_eye_gazes, capture_timestamp_ns)\n",
    "calibrated_eye_gaze = get_nearest_eye_gaze(calibrated_eye_gazes, capture_timestamp_ns)\n",
    "# get projection function\n",
    "device_calibration = provider.get_device_calibration()\n",
    "camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "assert camera_calibration is not None, \"no camera calibration\"\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 10))\n",
    "\n",
    "# Draw a cross at the projected gaze center location on the RGB image at available depth or if unavailable a 1m proxy\n",
    "depth_m = generalized_eye_gaze.depth or 1.0\n",
    "generalized_gaze_center_in_pixels = get_gaze_vector_reprojection(generalized_eye_gaze, rgb_stream_label, device_calibration, camera_calibration, depth_m)\n",
    "if generalized_gaze_center_in_pixels is not None:\n",
    "    ax1.imshow(image)\n",
    "    ax1.plot(generalized_gaze_center_in_pixels[0], generalized_gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
    "    ax1.grid(False)\n",
    "    ax1.axis(False)\n",
    "    ax1.set_title(\"Generalized Eye Gaze\")\n",
    "else:\n",
    "    print(f\"Eye gaze center projected to {generalized_gaze_center_in_pixels}, which is out of camera sensor plane.\")\n",
    "\n",
    "depth_m = calibrated_eye_gaze.depth or 1.0\n",
    "calibrated_gaze_center_in_pixels = get_gaze_vector_reprojection(calibrated_eye_gaze, rgb_stream_label, device_calibration, camera_calibration, depth_m = 1.0)\n",
    "if calibrated_gaze_center_in_pixels is not None:\n",
    "    ax2.imshow(image)\n",
    "    ax2.plot(calibrated_gaze_center_in_pixels[0], calibrated_gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
    "    ax2.grid(False)\n",
    "    ax2.axis(False)\n",
    "    ax2.set_title(\"Personalized Eye Gaze\")\n",
    "else:\n",
    "    print(f\"Eye gaze center projected to {calibrated_gaze_center_in_pixels}, which is out of camera sensor plane.\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab51e157",
   "metadata": {},
   "source": [
    "## Visualize hand tracking results (2d projections of landmarks, wrist and palm normals) on RGB and SLAM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57249d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "from projectaria_tools.core.calibration import CameraCalibration, DeviceCalibration\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "\n",
    "time_domain: TimeDomain = TimeDomain.DEVICE_TIME\n",
    "time_query_closest: TimeQueryOptions = TimeQueryOptions.CLOSEST\n",
    "\n",
    "NORMAL_VIS_LEN = 0.05  # meters\n",
    "\n",
    "# Get stream ids, stream labels, stream timestamps, and camera calibrations for RGB and SLAM cameras\n",
    "stream_ids: Dict[str, StreamId] = {\n",
    "    \"rgb\": StreamId(\"214-1\"),\n",
    "    \"slam-left\": StreamId(\"1201-1\"),\n",
    "    \"slam-right\": StreamId(\"1201-2\"),\n",
    "}\n",
    "stream_labels: Dict[str, str] = {\n",
    "    key: provider.get_label_from_stream_id(stream_id)\n",
    "    for key, stream_id in stream_ids.items()\n",
    "}\n",
    "stream_timestamps_ns: Dict[str, List[int]] = {\n",
    "    key: provider.get_timestamps_ns(stream_id, time_domain)\n",
    "    for key, stream_id in stream_ids.items()\n",
    "}\n",
    "camera_calibrations = {\n",
    "    key: device_calibration.get_camera_calib(stream_label)\n",
    "    for key, stream_label in stream_labels.items()\n",
    "}\n",
    "for key, camera_calibration in camera_calibrations.items():\n",
    "    assert camera_calibration is not None, f\"no camera calibration for {key}\"\n",
    "\n",
    "# Get device calibration and transform from device to sensor\n",
    "device_calibration = provider.get_device_calibration()\n",
    "\n",
    "\n",
    "def get_T_device_sensor(key: str):\n",
    "    return device_calibration.get_transform_device_sensor(stream_labels[key])\n",
    "\n",
    "\n",
    "# Get a sample frame for each of the RGB, SLAM left, and SLAM right streams\n",
    "sample_timestamp_ns: int = stream_timestamps_ns[\"rgb\"][120]\n",
    "sample_frames = {\n",
    "    key: provider.get_image_data_by_time_ns(\n",
    "        stream_id, sample_timestamp_ns, time_domain, time_query_closest\n",
    "    )[0]\n",
    "    for key, stream_id in stream_ids.items()\n",
    "}\n",
    "\n",
    "# Get the hand tracking pose\n",
    "hand_tracking_result = mps_data_provider.get_hand_tracking_result(\n",
    "    sample_timestamp_ns, time_query_closest\n",
    ")\n",
    "\n",
    "\n",
    "# Helper functions for reprojection and plotting\n",
    "def get_point_reprojection(\n",
    "    point_position_device: np.array, key: str\n",
    ") -> Optional[np.array]:\n",
    "    point_position_camera = get_T_device_sensor(key).inverse() @ point_position_device\n",
    "    point_position_pixel = camera_calibrations[key].project(point_position_camera)\n",
    "    return point_position_pixel\n",
    "\n",
    "\n",
    "def get_landmark_pixels(key: str) -> np.array:\n",
    "    left_wrist = None\n",
    "    left_palm = None\n",
    "    left_landmarks = None\n",
    "    right_wrist = None\n",
    "    right_palm = None\n",
    "    right_landmarks = None\n",
    "    left_wrist_normal_tip = None\n",
    "    left_palm_normal_tip = None\n",
    "    right_wrist_normal_tip = None\n",
    "    right_palm_normal_tip = None\n",
    "    if hand_tracking_result.left_hand:\n",
    "        left_landmarks = [\n",
    "            get_point_reprojection(landmark, key)\n",
    "            for landmark in hand_tracking_result.left_hand.landmark_positions_device\n",
    "        ]\n",
    "        left_wrist = get_point_reprojection(\n",
    "            hand_tracking_result.left_hand.landmark_positions_device[\n",
    "                int(mps.hand_tracking.HandLandmark.WRIST)\n",
    "            ],\n",
    "            key,\n",
    "        )\n",
    "        left_palm = get_point_reprojection(\n",
    "            hand_tracking_result.left_hand.landmark_positions_device[\n",
    "                int(mps.hand_tracking.HandLandmark.PALM_CENTER)\n",
    "            ],\n",
    "            key,\n",
    "        )\n",
    "        if hand_tracking_result.left_hand.wrist_and_palm_normal_device is not None:\n",
    "            left_wrist_normal_tip = get_point_reprojection(\n",
    "                hand_tracking_result.left_hand.landmark_positions_device[\n",
    "                    int(mps.hand_tracking.HandLandmark.WRIST)\n",
    "                ]\n",
    "                + hand_tracking_result.left_hand.wrist_and_palm_normal_device.wrist_normal_device\n",
    "                * NORMAL_VIS_LEN,\n",
    "                key,\n",
    "            )\n",
    "            left_palm_normal_tip = get_point_reprojection(\n",
    "                hand_tracking_result.left_hand.landmark_positions_device[\n",
    "                    int(mps.hand_tracking.HandLandmark.PALM_CENTER)\n",
    "                ]\n",
    "                + hand_tracking_result.left_hand.wrist_and_palm_normal_device.palm_normal_device\n",
    "                * NORMAL_VIS_LEN,\n",
    "                key,\n",
    "            )\n",
    "    if hand_tracking_result.right_hand:\n",
    "        right_landmarks = [\n",
    "            get_point_reprojection(landmark, key)\n",
    "            for landmark in hand_tracking_result.right_hand.landmark_positions_device\n",
    "        ]\n",
    "        right_wrist = get_point_reprojection(\n",
    "            hand_tracking_result.right_hand.landmark_positions_device[\n",
    "                int(mps.hand_tracking.HandLandmark.WRIST)\n",
    "            ],\n",
    "            key,\n",
    "        )\n",
    "        right_palm = get_point_reprojection(\n",
    "            hand_tracking_result.right_hand.landmark_positions_device[\n",
    "                int(mps.hand_tracking.HandLandmark.PALM_CENTER)\n",
    "            ],\n",
    "            key,\n",
    "        )\n",
    "        if hand_tracking_result.right_hand.wrist_and_palm_normal_device is not None:\n",
    "            right_wrist_normal_tip = get_point_reprojection(\n",
    "                hand_tracking_result.right_hand.landmark_positions_device[\n",
    "                    int(mps.hand_tracking.HandLandmark.WRIST)\n",
    "                ]\n",
    "                + hand_tracking_result.right_hand.wrist_and_palm_normal_device.wrist_normal_device\n",
    "                * NORMAL_VIS_LEN,\n",
    "                key,\n",
    "            )\n",
    "            right_palm_normal_tip = get_point_reprojection(\n",
    "                hand_tracking_result.right_hand.landmark_positions_device[\n",
    "                    int(mps.hand_tracking.HandLandmark.PALM_CENTER)\n",
    "                ]\n",
    "                + hand_tracking_result.right_hand.wrist_and_palm_normal_device.palm_normal_device\n",
    "                * NORMAL_VIS_LEN,\n",
    "                key,\n",
    "            )\n",
    "    \n",
    "    return (\n",
    "        left_wrist,\n",
    "        left_palm,\n",
    "        right_wrist,\n",
    "        right_palm,\n",
    "        left_wrist_normal_tip,\n",
    "        left_palm_normal_tip,\n",
    "        right_wrist_normal_tip,\n",
    "        right_palm_normal_tip,\n",
    "        left_landmarks,\n",
    "        right_landmarks\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_landmarks_and_connections(\n",
    "    plt, \n",
    "    left_landmarks, \n",
    "    right_landmarks, \n",
    "    connections, \n",
    "    img_height\n",
    "):\n",
    "    def plot_point(point, color):\n",
    "        plt.plot(img_height - 0.5 - point[1], point[0] + 0.5, \".\", c=color, mew=1, ms=5)\n",
    "    def plot_line(point1, point2, color):\n",
    "        plt.plot([img_height - 0.5 - point1[1], img_height - 0.5 - point2[1]], [point1[0] + 0.5, point2[0] + 0.5], color=color)\n",
    "\n",
    "    if left_landmarks:\n",
    "        for left_landmark in left_landmarks:\n",
    "            if left_landmark is not None:\n",
    "                plot_point(left_landmark, \"blue\")    \n",
    "        for connection in connections:\n",
    "            if left_landmarks[int(connection[0])] is not None and left_landmarks[int(connection[1])] is not None:\n",
    "                plot_line(left_landmarks[int(connection[0])], left_landmarks[int(connection[1])], \"blue\")\n",
    "    if right_landmarks:\n",
    "        for right_landmark in right_landmarks:\n",
    "            if right_landmark is not None:\n",
    "                plot_point(right_landmark, \"red\")    \n",
    "        for connection in connections:\n",
    "            if right_landmarks[int(connection[0])] is not None and right_landmarks[int(connection[1])] is not None:\n",
    "                plot_line(right_landmarks[int(connection[0])], right_landmarks[int(connection[1])], \"red\")\n",
    "\n",
    "\n",
    "def plot_wrists_and_palms(\n",
    "    plt,\n",
    "    left_wrist,\n",
    "    left_palm,\n",
    "    right_wrist,\n",
    "    right_palm,\n",
    "    left_wrist_normal_tip,\n",
    "    left_palm_normal_tip,\n",
    "    right_wrist_normal_tip,\n",
    "    right_palm_normal_tip,\n",
    "    img_height\n",
    "):\n",
    "    def plot_point(point, color):\n",
    "        plt.plot(img_height - 0.5 - point[1], point[0] + 0.5, \".\", c=color, mew=1, ms=15)\n",
    "\n",
    "    def plot_arrow(point, vector, color):\n",
    "        plt.arrow(img_height - 0.5 - point[1], point[0] + 0.5, -vector[1], vector[0], color=color)\n",
    "\n",
    "    if left_wrist is not None:\n",
    "        plot_point(left_wrist, \"blue\")\n",
    "    if left_palm is not None:\n",
    "        plot_point(left_palm, \"blue\")\n",
    "    if right_wrist is not None:\n",
    "        plot_point(right_wrist, \"red\")\n",
    "    if right_palm is not None:\n",
    "        plot_point(right_palm, \"red\")\n",
    "    if left_wrist_normal_tip is not None and left_wrist is not None:\n",
    "        plot_arrow(left_wrist, left_wrist_normal_tip - left_wrist, \"blue\")\n",
    "    if left_palm_normal_tip is not None and left_palm is not None:\n",
    "        plot_arrow(left_palm, left_palm_normal_tip - left_palm, \"blue\")\n",
    "    if right_wrist_normal_tip is not None and right_wrist is not None:\n",
    "        plot_arrow(right_wrist, right_wrist_normal_tip - right_wrist, \"red\")\n",
    "    if right_palm_normal_tip is not None and right_palm is not None:\n",
    "        plot_arrow(right_palm, right_palm_normal_tip - right_palm, \"red\")\n",
    "\n",
    "\n",
    "# Display wrist and palm positions on RGB, SLAM left, and SLAM right images\n",
    "plt.figure()\n",
    "rgb_image = sample_frames[\"rgb\"].to_numpy_array()\n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.rot90(rgb_image, -1))\n",
    "(\n",
    "    left_wrist,\n",
    "    left_palm,\n",
    "    right_wrist,\n",
    "    right_palm,\n",
    "    left_wrist_normal,\n",
    "    left_palm_normal,\n",
    "    right_wrist_normal,\n",
    "    right_palm_normal,\n",
    "    left_landmarks,\n",
    "    right_landmarks,\n",
    ") = get_landmark_pixels(\"rgb\")\n",
    "plot_wrists_and_palms(\n",
    "    plt,\n",
    "    left_wrist,\n",
    "    left_palm,\n",
    "    right_wrist,\n",
    "    right_palm,\n",
    "    left_wrist_normal,\n",
    "    left_palm_normal,\n",
    "    right_wrist_normal,\n",
    "    right_palm_normal,\n",
    "    rgb_image.shape[0]\n",
    ")\n",
    "plot_landmarks_and_connections(\n",
    "    plt,\n",
    "    left_landmarks,\n",
    "    right_landmarks,\n",
    "    mps.hand_tracking.kHandJointConnections,\n",
    "    rgb_image.shape[0]\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "slam_left_image = sample_frames[\"slam-left\"].to_numpy_array()\n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.rot90(slam_left_image, -1), cmap=\"gray\", vmin=0, vmax=255)\n",
    "(\n",
    "    left_wrist,\n",
    "    left_palm,\n",
    "    right_wrist,\n",
    "    right_palm,\n",
    "    left_wrist_normal,\n",
    "    left_palm_normal,\n",
    "    right_wrist_normal,\n",
    "    right_palm_normal,\n",
    "    left_landmarks,\n",
    "    right_landmarks,\n",
    ") = get_landmark_pixels(\"slam-left\")\n",
    "plot_wrists_and_palms(\n",
    "    plt,\n",
    "    left_wrist,\n",
    "    left_palm,\n",
    "    right_wrist,\n",
    "    right_palm,\n",
    "    left_wrist_normal,\n",
    "    left_palm_normal,\n",
    "    right_wrist_normal,\n",
    "    right_palm_normal,\n",
    "    slam_left_image.shape[0]\n",
    ")\n",
    "plot_landmarks_and_connections(\n",
    "    plt,\n",
    "    left_landmarks,\n",
    "    right_landmarks,\n",
    "    mps.hand_tracking.kHandJointConnections,\n",
    "    slam_left_image.shape[0]\n",
    ")\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "slam_right_image = sample_frames[\"slam-right\"].to_numpy_array()\n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.rot90(slam_right_image, -1), interpolation=\"nearest\", cmap=\"gray\")\n",
    "(\n",
    "    left_wrist,\n",
    "    left_palm,\n",
    "    right_wrist,\n",
    "    right_palm,\n",
    "    left_wrist_normal,\n",
    "    left_palm_normal,\n",
    "    right_wrist_normal,\n",
    "    right_palm_normal,\n",
    "    left_landmarks,\n",
    "    right_landmarks,\n",
    ") = get_landmark_pixels(\"slam-right\")\n",
    "plot_wrists_and_palms(\n",
    "    plt,\n",
    "    left_wrist,\n",
    "    left_palm,\n",
    "    right_wrist,\n",
    "    right_palm,\n",
    "    left_wrist_normal,\n",
    "    left_palm_normal,\n",
    "    right_wrist_normal,\n",
    "    right_palm_normal,\n",
    "    slam_right_image.shape[0]\n",
    ")\n",
    "plot_landmarks_and_connections(\n",
    "    plt,\n",
    "    left_landmarks,\n",
    "    right_landmarks,\n",
    "    mps.hand_tracking.kHandJointConnections,\n",
    "    slam_right_image.shape[0]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7692a2-e2b7-476b-a386-b77c38bed87f",
   "metadata": {},
   "source": [
    "## Examples for using the Online calibration from MPS\n",
    "\n",
    "`online_calibration.jsonl` contains one json online calibration record per line. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs. Note that after the v1.1.0 MPS SLAM release, we improved the RGB camera online calibration for time offsets estimation, intrinsics/extrinsics estimation, as well as exposing the image readout time for compensating the rolling shutter effect.\n",
    "\n",
    "The following example shows how to read the online calibrated parameters, and the difference of an RGB image undistorted with and without online calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70514a93-5277-44cb-9fd3-ef05187317c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import calibration\n",
    "\n",
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "rgb_stream_label = provider.get_label_from_stream_id(rgb_stream_id)\n",
    "num_rgb_frames = provider.get_num_data(rgb_stream_id)\n",
    "rgb_frame = provider.get_image_data_by_index(rgb_stream_id, (int)(num_rgb_frames / 2))\n",
    "assert rgb_frame[0] is not None, \"no rgb frame\"\n",
    "\n",
    "image = rgb_frame[0].to_numpy_array()\n",
    "capture_timestamp_ns = rgb_frame[1].capture_timestamp_ns\n",
    "\n",
    "# get the online calibration RGB projection function\n",
    "corrected_rgb_timestamp_ns = mps_data_provider.get_rgb_corrected_timestamp_ns(capture_timestamp_ns)\n",
    "\n",
    "print(\"The online calibration estimated better timestamp for rgb mid exposure\",\n",
    "      f\"time is {corrected_rgb_timestamp_ns}ns vs {capture_timestamp_ns}ns with\",\n",
    "      f\"offset {corrected_rgb_timestamp_ns - capture_timestamp_ns}ns\")\n",
    "\n",
    "\n",
    "# Example API to get the online calibrated RGB pose\n",
    "corrected_rgb_pose = mps_data_provider.get_rgb_corrected_closed_loop_pose(capture_timestamp_ns)\n",
    "\n",
    "# Get the online calibration for rgb camera and undistort the image using it\n",
    "online_camera_calibration = mps_data_provider.get_online_calibration(capture_timestamp_ns).get_camera_calib(rgb_stream_label)\n",
    "rgb_linear_camera_calibration_online = calibration.get_linear_camera_calibration(\n",
    "    int(online_camera_calibration.get_image_size()[0]),\n",
    "    int(online_camera_calibration.get_image_size()[1]),\n",
    "    online_camera_calibration.get_focal_lengths()[0],\n",
    "    \"pinhole\",\n",
    "    online_camera_calibration.get_transform_device_camera(),\n",
    ")\n",
    "undistort_image_online = calibration.distort_by_calibration(\n",
    "                image,\n",
    "                rgb_linear_camera_calibration_online,\n",
    "                online_camera_calibration,\n",
    "            )\n",
    "\n",
    "# get projection function from device factory calibration and undistort the image using it\n",
    "device_calibration = provider.get_device_calibration()\n",
    "camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "rgb_linear_camera_calibration = calibration.get_linear_camera_calibration(\n",
    "    int(camera_calibration.get_image_size()[0]),\n",
    "    int(camera_calibration.get_image_size()[1]),\n",
    "    camera_calibration.get_focal_lengths()[0],\n",
    "    \"pinhole\",\n",
    "    camera_calibration.get_transform_device_camera(),\n",
    ")\n",
    "undistort_image = calibration.distort_by_calibration(\n",
    "                image,\n",
    "                rgb_linear_camera_calibration,\n",
    "                camera_calibration,\n",
    "            )\n",
    "\n",
    "# Compute the difference between the two images\n",
    "diff_image = undistort_image - undistort_image_online\n",
    "# Create a figure with three subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "# Plot the first image\n",
    "axs[0].imshow(undistort_image)\n",
    "axs[0].set_title('Undistorted by the device calibration')\n",
    "# Plot the second image\n",
    "axs[1].imshow(undistort_image_online)\n",
    "axs[1].set_title('Undistorted by the online calibration')\n",
    "# Plot the difference image\n",
    "axs[2].imshow(diff_image)\n",
    "axs[2].set_title('Difference')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Note that the rgb image read out time is available now for rolling shutter handling\n",
    "print(f\"Rgb camera take {online_camera_calibration.get_readout_time_sec()} seconds to read out the image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
